# ğŸ§  EXPLAINIUM - Advanced AI-Powered Knowledge Extraction System

EXPLAINIUM has been transformed from a basic entity extraction system into a sophisticated, AI-powered knowledge processing platform that extracts deep, meaningful insights from company documents and tacit knowledge.

## ğŸš€ Key Features

### **Deep Knowledge Extraction**
- **Multi-pass AI Analysis**: Uses local LLMs (Mistral-7B, Phi-2, TinyLlama) for comprehensive understanding
- **Contextual Processing**: Understands document purpose, business context, and domain-specific patterns
- **Tacit Knowledge Discovery**: Identifies implicit workflows, organizational structures, and unstated operational patterns
- **Operational Intelligence**: Extracts SOPs, decision criteria, compliance requirements, and risk factors

### **Advanced AI Models**
- **Primary LLM**: Mistral-7B-Instruct-v0.2 (4-bit quantized for 16GB RAM)
- **Embeddings**: BAAI/bge-small-en-v1.5 for semantic search
- **Document Understanding**: Microsoft LayoutLMv3 for structured documents
- **Vision**: Salesforce BLIP for image understanding
- **Audio**: Whisper + Pyannote for transcription and speaker diarization

### **Knowledge Graph Architecture**
- **In-Memory Graph**: Neo4jLiteGraph for interconnected knowledge representation
- **Node Types**: Concepts, People, Processes, Systems, Requirements, Risks
- **Relationship Types**: Dependencies, Workflows, Hierarchies, Associations
- **Real-time Updates**: Dynamic graph building as new knowledge is extracted

### **Apple M4 Optimization**
- **Metal Acceleration**: Leverages Apple Silicon neural engine
- **Memory Management**: 4-bit quantization, dynamic batching, lazy loading
- **Performance Monitoring**: Real-time RAM usage and processing speed tracking
- **Hardware Profiles**: Automatic detection and optimization for 16GB/32GB configurations

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXPLAINIUM System                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend Layer                                            â”‚
â”‚  â”œâ”€â”€ Knowledge Table (Streamlit)                          â”‚
â”‚  â”œâ”€â”€ Interactive Visualizations                           â”‚
â”‚  â””â”€â”€ Export & Documentation                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Processing Layer                                          â”‚
â”‚  â”œâ”€â”€ Advanced Knowledge Engine                            â”‚
â”‚  â”œâ”€â”€ Intelligent Document Processor                       â”‚
â”‚  â””â”€â”€ Multi-modal Content Handler                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AI Model Layer                                            â”‚
â”‚  â”œâ”€â”€ Local LLMs (Mistral-7B, Phi-2, TinyLlama)           â”‚
â”‚  â”œâ”€â”€ Embedding Models (BGE-small)                         â”‚
â”‚  â”œâ”€â”€ Vision Models (BLIP)                                 â”‚
â”‚  â””â”€â”€ Audio Models (Whisper + Pyannote)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Knowledge Layer                                           â”‚
â”‚  â”œâ”€â”€ Neo4jLiteGraph                                       â”‚
â”‚  â”œâ”€â”€ Semantic Search                                       â”‚
â”‚  â””â”€â”€ Relationship Mapping                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Optimization Layer                                        â”‚
â”‚  â”œâ”€â”€ Model Caching (DiskCache)                            â”‚
â”‚  â”œâ”€â”€ M4-specific Optimizations                            â”‚
â”‚  â”œâ”€â”€ Streaming Processing                                 â”‚
â”‚  â””â”€â”€ Performance Monitoring                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

### **Hardware Requirements**
- **Minimum**: Apple M4 Mac with 16GB RAM
- **Recommended**: Apple M4 Mac with 32GB RAM
- **Storage**: 10GB+ for models and cache
- **Network**: Internet connection for initial model download

### **Software Requirements**
- **OS**: macOS 14.0+ (optimized for Apple Silicon)
- **Python**: 3.9+
- **RAM**: 16GB+ available for processing
- **GPU**: Apple Metal support (automatic)

## ğŸ› ï¸ Installation

### **1. Clone the Repository**
```bash
git clone <repository-url>
cd explainium
```

### **2. Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On macOS/Linux
# or
venv\Scripts\activate  # On Windows
```

### **3. Install Dependencies**
```bash
pip install -r requirements.txt
```

### **4. Download AI Models**
```bash
python scripts/model_manager.py --action setup
```

### **5. Run the System**
```bash
# Start the knowledge table frontend
streamlit run src/frontend/knowledge_table.py

# Or run the main processor
python -m src.processors.processor
```

## ğŸ”§ Configuration

### **AI Model Configuration**
The system automatically detects your hardware profile and configures models accordingly:

```python
# M4 16GB Profile
llm_model: "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
quantization: "Q4_K_M"
max_ram: "4GB"

# M4 32GB Profile  
llm_model: "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
quantization: "Q5_K_M"
max_ram: "8GB"
```

### **Performance Tuning**
```python
# src/core/config.py
@dataclass
class AIConfig:
    batch_size: int = 4  # Optimized for 16GB RAM
    chunk_size: int = 512
    chunk_overlap: int = 50
    use_gpu: bool = True  # Apple Metal acceleration
    quantization: str = "4bit"
```

## ğŸ“š Usage Examples

### **Basic Document Processing**
```python
from src.processors.processor import DocumentProcessor

# Initialize processor
processor = DocumentProcessor()

# Process a document
document = {
    "content": "Customer onboarding process documentation...",
    "type": "pdf",
    "metadata": {"department": "operations"}
}

# Extract deep knowledge
knowledge = await processor.process_document(document)
```

### **Contextual Processing**
```python
# Process with company context
company_context = {
    "industry": "healthcare",
    "size": "enterprise",
    "compliance": ["HIPAA", "SOC2"]
}

enhanced_knowledge = await processor.process_document_with_context(
    document, company_context
)
```

### **Tacit Knowledge Extraction**
```python
# Extract patterns across multiple documents
documents = [doc1, doc2, doc3, ...]
tacit_knowledge = await processor.extract_tacit_knowledge(documents)

# Results include:
# - Implicit workflows
# - Organizational structures  
# - Policy changes over time
# - Communication networks
```

### **Knowledge Graph Queries**
```python
from src.ai.advanced_knowledge_engine import AdvancedKnowledgeEngine

engine = AdvancedKnowledgeEngine()

# Find related concepts
related = engine.knowledge_graph.find_related_nodes("customer_onboarding")

# Search by type
processes = engine.knowledge_graph.find_nodes_by_type("process")

# Get workflow paths
workflow = engine.knowledge_graph.find_workflow_path("start", "end")
```

## ğŸ“Š Frontend Features

### **Knowledge Table Dashboard**
- **Large Data Table**: Display all extracted knowledge with search and filtering
- **Advanced Filters**: By type, confidence, date range, and search terms
- **Visual Analytics**: Charts showing knowledge distribution and trends
- **Interactive Graph**: Network visualization of knowledge relationships
- **Export Options**: CSV, JSON, Markdown, Cytoscape formats

### **Real-time Updates**
- Live updates as new knowledge is extracted
- Confidence scoring and validation
- Performance metrics and monitoring
- Memory usage tracking

## ğŸ” Model Management

### **Automatic Setup**
```bash
# Detect hardware and setup optimal models
python scripts/model_manager.py --action setup

# List installed models
python scripts/model_manager.py --action list

# Validate model integrity
python scripts/model_manager.py --action validate

# Clean up models
python scripts/model_manager.py --action cleanup
```

### **Manual Model Management**
```bash
# Setup for specific hardware profile
python scripts/model_manager.py --action setup --hardware-profile m4_32gb

# Clean specific model type
python scripts/model_manager.py --action cleanup --model-type llm
```

## ğŸ§ª Testing

### **Run Test Suite**
```bash
# Install test dependencies
pip install pytest pytest-asyncio

# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_advanced_knowledge_engine.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

### **Test Coverage**
- **Unit Tests**: Individual component testing
- **Integration Tests**: End-to-end workflow testing
- **Performance Tests**: Memory usage and speed validation
- **Model Tests**: AI model functionality validation

## ğŸ“ˆ Performance Metrics

### **Memory Usage**
- **Model Loading**: <4GB for primary LLM
- **Processing**: <8GB peak during extraction
- **Cache**: <2GB for disk-based caching
- **Total**: <16GB for 16GB Mac, <32GB for 32GB Mac

### **Processing Speed**
- **Document Processing**: 100-500 words/second
- **Knowledge Extraction**: 2-5 seconds per document
- **Graph Building**: Real-time updates
- **Search Queries**: <100ms response time

### **Scalability**
- **Document Size**: Up to 100MB per document
- **Batch Processing**: Configurable batch sizes
- **Concurrent Processing**: Async processing support
- **Memory Optimization**: Automatic model swapping

## ğŸš€ Deployment

### **Local Development**
```bash
# Development mode with hot reload
streamlit run src/frontend/knowledge_table.py --server.runOnSave true

# Run with debug logging
LOG_LEVEL=DEBUG python -m src.processors.processor
```

### **Production Deployment**
```bash
# Build optimized models
python scripts/model_manager.py --action setup --hardware-profile m4_32gb

# Run with production settings
export ENVIRONMENT=production
streamlit run src/frontend/knowledge_table.py --server.port 8501
```

### **Docker Support**
```dockerfile
# Dockerfile example
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "src/frontend/knowledge_table.py"]
```

## ğŸ”§ Troubleshooting

### **Common Issues**

#### **Memory Errors**
```bash
# Check available RAM
python -c "import psutil; print(psutil.virtual_memory())"

# Reduce batch size in config
batch_size: 2  # Instead of 4
```

#### **Model Loading Failures**
```bash
# Validate models
python scripts/model_manager.py --action validate

# Re-download corrupted models
python scripts/model_manager.py --action cleanup --model-type llm
python scripts/model_manager.py --action setup
```

#### **Performance Issues**
```bash
# Check Metal acceleration
python -c "import torch; print(torch.backends.mps.is_available())"

# Monitor performance
python -m src.core.optimization --monitor
```

### **Logging and Debugging**
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Check system resources
from src.core.optimization import PerformanceMonitor
monitor = PerformanceMonitor()
print(monitor.get_system_status())
```

## ğŸ¤ Contributing

### **Development Setup**
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

### **Code Standards**
- Follow PEP 8 style guidelines
- Add type hints for all functions
- Include docstrings for all classes and methods
- Write comprehensive tests
- Update documentation for API changes

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Mistral AI** for the Mistral-7B model
- **Microsoft** for Phi-2 and LayoutLMv3
- **BAAI** for the BGE embedding models
- **Salesforce** for BLIP vision models
- **Apple** for Metal Performance Shaders

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **Documentation**: [Wiki](https://github.com/your-repo/wiki)

---

**EXPLAINIUM** - Transforming document understanding through advanced AI and deep knowledge extraction. ğŸ§ âœ¨