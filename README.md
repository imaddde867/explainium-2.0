# ğŸ§  Explainium 2.0 - LLM-First Knowledge Extraction Platform

> **Revolutionary document processing with offline LLM intelligence as the primary source for superior knowledge extraction results.**

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://python.org)
[![LLM-First](https://img.shields.io/badge/AI-LLM--First-green.svg)](https://github.com)
[![Offline Processing](https://img.shields.io/badge/processing-offline-orange.svg)](https://github.com)

## ğŸš€ **Core Principles & Processing Rules**

### **THE FUNDAMENTAL LAWS OF EXPLAINIUM**

1. **ğŸ§  LLM SUPREMACY RULE**: Offline LLM models are the PRIMARY processing source (Confidence Threshold: â‰¥0.75)
2. **â­ QUALITY FIRST RULE**: High confidence thresholds ensure superior results (Production Threshold: â‰¥0.85)
3. **ğŸ”„ HIERARCHICAL FALLBACK RULE**: Graceful degradation through processing layers when needed
4. **âœ… VALIDATION REQUIRED RULE**: All extractions must pass validation thresholds (â‰¥0.70)
5. **ğŸ“Š STRUCTURED OUTPUT RULE**: Clean, categorized, database-ready results mandatory

### **Processing Hierarchy (In Order of Priority)**

```
ğŸ¥‡ PRIMARY:   LLM-First Processing Engine    (Confidence: 0.75-0.95)
ğŸ¥ˆ FALLBACK:  Enhanced Pattern Recognition   (Confidence: 0.60-0.80)  
ğŸ¥‰ EMERGENCY: Legacy Pattern Matching        (Confidence: 0.50-0.65)
```

## ğŸ“‹ **What Explainium Extracts & How**

### **ğŸ¯ Knowledge Categories Extracted**

| Category | Confidence | LLM Enhanced | Examples |
|----------|------------|--------------|----------|
| **ğŸ“Š Technical Specifications** | 0.95 | âœ… Yes | Equipment specs, parameters, measurements |
| **ğŸ›¡ï¸ Safety Requirements** | 0.90 | âœ… Yes | Hazards, protective equipment, procedures |
| **ğŸ”„ Process Intelligence** | 0.85 | âœ… Yes | Workflows, step-by-step procedures |
| **âš–ï¸ Compliance Governance** | 0.80 | âœ… Yes | Regulations, standards, requirements |
| **ğŸ‘¥ Organizational Data** | 0.75 | âœ… Yes | Roles, responsibilities, certifications |
| **ğŸ“š Knowledge Definitions** | 0.70 | âœ… Yes | Terms, definitions, explanations |

### **ğŸ­ LLM Processing Prompts (5 Specialized Types)**

1. **ğŸ”„ Key Processes**: Workflow extraction and process identification
2. **ğŸ›¡ï¸ Safety Requirements**: Hazard identification and risk mitigation measures  
3. **âš™ï¸ Technical Specifications**: Parameter extraction with units and ranges
4. **ğŸ“‹ Compliance Requirements**: Regulatory and standard identification
5. **ğŸ‘¥ Organizational Info**: Role and responsibility extraction

## ğŸ—ï¸ **Architecture Overview**

```
ğŸ“ explainium-2.0/
â”œâ”€â”€ ğŸ§  src/ai/                          # AI Processing Engines
â”‚   â”œâ”€â”€ llm_processing_engine.py        # PRIMARY: LLM-First Engine
â”‚   â”œâ”€â”€ enhanced_extraction_engine.py   # Pattern Recognition Engine  
â”‚   â”œâ”€â”€ knowledge_categorization_engine.py # Entity Classification
â”‚   â”œâ”€â”€ advanced_knowledge_engine.py    # Legacy Fallback Engine
â”‚   â””â”€â”€ document_intelligence_analyzer.py # Document Analysis
â”œâ”€â”€ ğŸ”§ src/processors/                  # Document Processing
â”‚   â””â”€â”€ processor.py                    # Main Document Processor
â”œâ”€â”€ ğŸ¨ src/frontend/                    # User Interface
â”‚   â””â”€â”€ knowledge_table.py             # Streamlit Dashboard
â”œâ”€â”€ ğŸ“Š src/database/                    # Data Management
â”‚   â”œâ”€â”€ models.py                       # Database Models
â”‚   â””â”€â”€ crud.py                         # Database Operations
â”œâ”€â”€ ğŸ¤– models/                          # AI Models (Offline)
â”‚   â”œâ”€â”€ llm/Mistral-7B-Instruct-v0.2/  # Primary LLM Model
â”‚   â”œâ”€â”€ embeddings/                     # Embedding Models
â”‚   â””â”€â”€ setup_config.json              # Model Configuration
â””â”€â”€ ğŸ“š documents_samples/               # Test Documents
```

## ğŸš€ **Quick Start**

### **Prerequisites**
- Python 3.12+
- 16GB+ RAM (for optimal LLM performance)
- macOS with Metal support (M-series chips) or CUDA GPU

### **Installation**

```bash
# Clone repository
git clone https://github.com/your-org/explainium-2.0.git
cd explainium-2.0

# Setup environment
chmod +x setup.sh
./setup.sh

# Start application
./start.sh
```

### **Access Points**
- **ğŸ¨ Frontend Dashboard**: http://localhost:8501
- **ğŸ”§ Backend API**: http://localhost:8000  
- **ğŸ“– API Documentation**: http://localhost:8000/docs

## ğŸ“Š **Performance Metrics**

### **Before vs After LLM-First Enhancement**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Entities Extracted** | 4 | 24 | **6x increase** |
| **Average Confidence** | 0.55 | 0.85 | **55% increase** |
| **Quality Rating** | 100% POOR | 80% EXCELLENT | **Massive improvement** |
| **Processing Methods** | 1 basic | 5 intelligent | **5x more methods** |

### **LLM-First Results Example**

From a simple 20-line industrial manual:
- **ğŸ“Š 6 Technical Specifications** (0.95 confidence)
- **ğŸ›¡ï¸ 3 Safety Requirements** (0.90 confidence)  
- **ğŸ‘¥ 5 Personnel Records** (0.85 confidence)
- **ğŸ”„ 3 Process Procedures** (0.80 confidence)
- **ğŸ“š 6 Knowledge Definitions** (0.75 confidence)

## ğŸ¯ **Key Features**

### **ğŸ§  LLM-First Intelligence**
- **Offline Mistral-7B** model for primary processing
- **Multi-prompt strategy** for comprehensive extraction
- **Intelligent relationship discovery** between entities
- **Context-aware semantic understanding**

### **ğŸ“Š Quality Assurance** 
- **Hierarchical processing** with automatic fallbacks
- **Validation gates** at every processing stage
- **Confidence scoring** with quality thresholds
- **Real-time quality metrics** and monitoring

### **ğŸ¨ Professional UI/UX**
- **LLM-First processing indicators** in dashboard
- **Dynamic filter generation** based on extracted types
- **Processing method visualization** (Primary/Fallback)
- **Quality confidence indicators** with color coding

### **ğŸ”§ Document Support**
- **Multi-format processing**: PDF, DOCX, TXT, Images, Audio, Video
- **Intelligent document type detection**
- **OCR and audio transcription** capabilities
- **Batch processing** with quality tracking

## ğŸ› ï¸ **Configuration**

### **LLM Model Settings**

```json
{
  "hardware_profile": "m4_16gb",
  "models": {
    "llm": {
      "path": "models/llm/Mistral-7B-Instruct-v0.2-GGUF",
      "quantization": "Q4_K_M",
      "context_length": 4096,
      "threads": 8
    }
  }
}
```

### **Quality Thresholds**

```python
# Processing Engine Thresholds
LLM_MINIMUM = 0.75          # Minimum LLM confidence
ENHANCED_MINIMUM = 0.60     # Enhanced extraction minimum  
COMBINED_MINIMUM = 0.80     # Combined analysis threshold
ENTITY_VALIDATION = 0.70    # Entity validation score
PRODUCTION_READY = 0.85     # Production deployment threshold
```

## ğŸ“ˆ **API Usage**

### **Process Document**

```python
from src.processors.processor import DocumentProcessor

processor = DocumentProcessor()
result = processor.process_document("/path/to/document.pdf", document_id=1)

# Access LLM-extracted knowledge
knowledge = result['knowledge']['extracted_entities']
processing_method = result['knowledge']['processing_metadata']['method']
confidence = result['knowledge']['processing_metadata']['confidence_score']
```

### **LLM-First Engine Direct**

```python
from src.ai.llm_processing_engine import LLMProcessingEngine

engine = LLMProcessingEngine()
await engine.initialize()

result = await engine.process_document(
    content="Your document content here",
    document_type="technical_manual",
    metadata={"filename": "manual.pdf"}
)

# Access comprehensive results
entities = result.entities
confidence = result.confidence_score
quality_metrics = result.quality_metrics
```

## ğŸ”¬ **Development & Testing**

### **Run Tests**
```bash
# Test LLM processing
python -c "
from src.ai.llm_processing_engine import LLMProcessingEngine
import asyncio

async def test():
    engine = LLMProcessingEngine()
    await engine.initialize()
    print('âœ… LLM Engine ready')

asyncio.run(test())
"
```

### **Quality Monitoring**
```bash
# Check processing statistics
python -c "
from src.processors.processor import DocumentProcessor
processor = DocumentProcessor()
if processor.llm_engine_available:
    stats = processor.llm_processing_engine.get_processing_summary()
    print(f'LLM Available: {stats[\"llm_available\"]}')
    print(f'Processing Rules: {stats[\"processing_rules_count\"]}')
"
```

## ğŸ“ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
