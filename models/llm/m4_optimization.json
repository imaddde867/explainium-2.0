{
  "model_path": "models/llm/Mistral-7B-Instruct-v0.2-GGUF",
  "metal_layers": -1,
  "threads": 8,
  "batch_size": 4,
  "context_length": 4096,
  "memory_settings": {},
  "quantization": "Q4_K_M",
  "applied": [
    "MLX framework available",
    "Thread count optimized: 8",
    "Memory settings optimized"
  ]
}